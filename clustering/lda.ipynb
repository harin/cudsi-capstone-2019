{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import gc\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"pubId\", \"is_hourly\", \"seqId\", \"on_homepage\", \"canonicalUrl\",\n",
    "                   \"firstScrape\", \"lang_iso\", \"lang_reliability\", \"title\", \"text\"]\n",
    "\n",
    "def read_article_df(file):\n",
    "    articles_dt = file.read().split('\\n')[:-1]\n",
    "    pubId, canonicalUrl,firstScrape,title,text,lang_reliability = [],[],[],[],[],[]\n",
    "    lang_iso = []\n",
    "    for article in articles_dt:    \n",
    "        row = article.split('\\t')\n",
    "        pubId.append(row[0])\n",
    "        canonicalUrl.append(row[4])\n",
    "        firstScrape.append(row[5])\n",
    "        lang_iso.append(row[6])\n",
    "        lang_reliability.append(row[7])\n",
    "        title.append(row[8])\n",
    "        text.append(row[9])\n",
    "\n",
    "    articles_df = pd.DataFrame()\n",
    "    articles_df['pubId'] = pubId\n",
    "    articles_df['canonicalUrl'] = canonicalUrl\n",
    "    articles_df['firstScrape'] = firstScrape\n",
    "    articles_df['title'] = title\n",
    "    articles_df['text'] = text\n",
    "    articles_df['lang_reliability'] = lang_reliability\n",
    "    articles_df['lang_iso'] = lang_iso\n",
    "    return articles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df = read_article_df(open('../data/raw/2018_07_19_04_59_08/articles.txt', encoding='utf-8'))\n",
    "article_df = article_df[article_df.lang_reliability == '1'].reset_index(drop=True)\n",
    "article_df['title'] = article_df.title.apply(lambda s: s.strip())\n",
    "article_df['text'] = article_df.text.apply(lambda s: s.strip())\n",
    "article_df['title_len'] = article_df.title.apply(len)\n",
    "article_df['text_len'] = article_df.text.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(176664, 9)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df = article_df[article_df.title_len > 0]\n",
    "clean_df = clean_df[clean_df.text_len > 100]\n",
    "clean_df = clean_df[clean_df.lang_iso == 'en']\n",
    "clean_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>canonicalUrl</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>bbc.com/news/av/world-asia-44875089/thai-cave-...</td>\n",
       "      <td>cave_rescue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>indystar.com/story/news/nation-now/2018/07/16/...</td>\n",
       "      <td>cave_rescue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>washingtonpost.com/world/asia_pacific/these-di...</td>\n",
       "      <td>cave_rescue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>au.news.yahoo.com/navy-seal-died-thai-cave-res...</td>\n",
       "      <td>cave_rescue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>yahoo.com/news/m/8adca8cd-6cc3-307c-b109-9cd1d...</td>\n",
       "      <td>cave_rescue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>487</td>\n",
       "      <td>chicago.suntimes.com/news/military-veterans-di...</td>\n",
       "      <td>helsinki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>488</td>\n",
       "      <td>dailycaller.com/2018/07/19/mike-huckabee-media...</td>\n",
       "      <td>helsinki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>489</td>\n",
       "      <td>sfgate.com/news/politics/article/trump-embrace...</td>\n",
       "      <td>helsinki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>yahoo.com/news/m/78f6000e-d04c-355b-867d-8d5c8...</td>\n",
       "      <td>helsinki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>491</td>\n",
       "      <td>chicagotribune.com/news/local/politics/ct-met-...</td>\n",
       "      <td>helsinki</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>775 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          canonicalUrl        label\n",
       "0    bbc.com/news/av/world-asia-44875089/thai-cave-...  cave_rescue\n",
       "1    indystar.com/story/news/nation-now/2018/07/16/...  cave_rescue\n",
       "2    washingtonpost.com/world/asia_pacific/these-di...  cave_rescue\n",
       "3    au.news.yahoo.com/navy-seal-died-thai-cave-res...  cave_rescue\n",
       "4    yahoo.com/news/m/8adca8cd-6cc3-307c-b109-9cd1d...  cave_rescue\n",
       "..                                                 ...          ...\n",
       "487  chicago.suntimes.com/news/military-veterans-di...     helsinki\n",
       "488  dailycaller.com/2018/07/19/mike-huckabee-media...     helsinki\n",
       "489  sfgate.com/news/politics/article/trump-embrace...     helsinki\n",
       "490  yahoo.com/news/m/78f6000e-d04c-355b-867d-8d5c8...     helsinki\n",
       "491  chicagotribune.com/news/local/politics/ct-met-...     helsinki\n",
       "\n",
       "[775 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label1 = pd.read_csv('../data/raw/labels/cave_rescue/lower_bound.txt', header=None)\n",
    "label1.columns = ['canonicalUrl']\n",
    "label1['label'] = 'cave_rescue'\n",
    "\n",
    "label2 = pd.read_csv('../data/raw/labels/duckboat/lower_bound.txt', header=None)\n",
    "label2.columns = ['canonicalUrl']\n",
    "label2['label'] = 'duckboat'\n",
    "\n",
    "label3 = pd.read_csv('../data/raw/labels/helsinki_summit/lower_bound.txt', header=None)\n",
    "label3.columns = ['canonicalUrl']\n",
    "label3['label'] = 'helsinki'\n",
    "\n",
    "label_df = pd.concat([label1, label2, label3])\n",
    "label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = clean_df.merge(label_df, on='canonicalUrl', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((140, 10), (176524, 10))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_with_label = clean_df[~clean_df.label.isna()]\n",
    "article_without_label = clean_df[clean_df.label.isna()]\n",
    "article_with_label.shape, article_without_label.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = pd.concat([article_with_label, article_without_label.sample(10000)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pubId</th>\n",
       "      <th>canonicalUrl</th>\n",
       "      <th>firstScrape</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>lang_reliability</th>\n",
       "      <th>lang_iso</th>\n",
       "      <th>title_len</th>\n",
       "      <th>text_len</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>290</td>\n",
       "      <td>zerohedge.com/news/2018-07-19/fbi-chief-threat...</td>\n",
       "      <td>7/19/2018 8:26:52 AM -04:00</td>\n",
       "      <td>FBI Chief Threatens To Quit If Trump Invites R...</td>\n",
       "      <td>by Knave Dave - Jul 18, 2018 1:11 pm ### This ...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>78</td>\n",
       "      <td>2858</td>\n",
       "      <td>helsinki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10696</td>\n",
       "      <td>33</td>\n",
       "      <td>washingtonpost.com/news/morning-mix/wp/2018/07...</td>\n",
       "      <td>7/19/2018 11:51:57 PM -04:00</td>\n",
       "      <td>At least 8 reported dead as duck boat sinks ne...</td>\n",
       "      <td>At least 8 reported dead as duck boat sinks ne...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>90</td>\n",
       "      <td>856</td>\n",
       "      <td>duckboat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10801</td>\n",
       "      <td>33</td>\n",
       "      <td>washingtonpost.com/news/posteverything/wp/2018...</td>\n",
       "      <td>7/19/2018 6:27:03 AM -04:00</td>\n",
       "      <td>Ukraineâ€™s not a country, Putin told Bush. What...</td>\n",
       "      <td>PostEverything Perspective ### Perspective Int...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>79</td>\n",
       "      <td>8487</td>\n",
       "      <td>helsinki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13312</td>\n",
       "      <td>237</td>\n",
       "      <td>hotair.com/archives/2018/07/19/looking-glass-d...</td>\n",
       "      <td>7/19/2018 1:35:59 PM -04:00</td>\n",
       "      <td>Through the looking glass: Democrats attack \"R...</td>\n",
       "      <td>Through the looking glass: Democrats attack â€œR...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>86</td>\n",
       "      <td>4440</td>\n",
       "      <td>helsinki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16299</td>\n",
       "      <td>118</td>\n",
       "      <td>philly.com/philly/news/nation_world/20180719_a...</td>\n",
       "      <td>7/19/2018 11:15:42 PM -04:00</td>\n",
       "      <td>Sheriff: 8 people dead after Missouri tourist ...</td>\n",
       "      <td>Sheriff: 8 people dead after Missouri tourist ...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>68</td>\n",
       "      <td>766</td>\n",
       "      <td>duckboat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119207</td>\n",
       "      <td>100</td>\n",
       "      <td>nordic.businessinsider.com/roger-federer-lost-...</td>\n",
       "      <td>7/19/2018 11:45:25 AM -04:00</td>\n",
       "      <td>Roger Federer lost his iconic 'RF' logo when h...</td>\n",
       "      <td>Lifestyle ### * Copyright Â© 2018 Business Insi...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>107</td>\n",
       "      <td>1720</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23450</td>\n",
       "      <td>20</td>\n",
       "      <td>reddit.com/user/diglos76</td>\n",
       "      <td>7/19/2018 7:05:52 AM -04:00</td>\n",
       "      <td>diglos76 (u/diglos76) - Reddit</td>\n",
       "      <td>Press J to jump to the feed. Press question ma...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>30</td>\n",
       "      <td>6605</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44755</td>\n",
       "      <td>1433</td>\n",
       "      <td>record-eagle.com/news/go/mike-terrell-manistee...</td>\n",
       "      <td>7/19/2018 8:46:10 PM -04:00</td>\n",
       "      <td>Mike Terrell: Manistee River offers small stre...</td>\n",
       "      <td>A few passing clouds. Low 68F. Winds light and...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>80</td>\n",
       "      <td>4808</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172391</td>\n",
       "      <td>974</td>\n",
       "      <td>wgntv.com/2018/07/18/t-storms-coming-back-onto...</td>\n",
       "      <td>7/19/2018 2:43:18 PM -04:00</td>\n",
       "      <td>T-storms coming back onto Chicagoâ€™s weather sc...</td>\n",
       "      <td>T-storms coming back onto Chicagoâ€™s weather sc...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>170</td>\n",
       "      <td>248</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57828</td>\n",
       "      <td>1409</td>\n",
       "      <td>actionnewsjax.com/news/trending-now/tire-comes...</td>\n",
       "      <td>7/20/2018 3:05:12 AM -04:00</td>\n",
       "      <td>Tire comes off van on Ohio interstate, crashes...</td>\n",
       "      <td>### Thank you for registering! ### We have sen...</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>95</td>\n",
       "      <td>339</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10140 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pubId                                       canonicalUrl  \\\n",
       "62       290  zerohedge.com/news/2018-07-19/fbi-chief-threat...   \n",
       "10696     33  washingtonpost.com/news/morning-mix/wp/2018/07...   \n",
       "10801     33  washingtonpost.com/news/posteverything/wp/2018...   \n",
       "13312    237  hotair.com/archives/2018/07/19/looking-glass-d...   \n",
       "16299    118  philly.com/philly/news/nation_world/20180719_a...   \n",
       "...      ...                                                ...   \n",
       "119207   100  nordic.businessinsider.com/roger-federer-lost-...   \n",
       "23450     20                           reddit.com/user/diglos76   \n",
       "44755   1433  record-eagle.com/news/go/mike-terrell-manistee...   \n",
       "172391   974  wgntv.com/2018/07/18/t-storms-coming-back-onto...   \n",
       "57828   1409  actionnewsjax.com/news/trending-now/tire-comes...   \n",
       "\n",
       "                         firstScrape  \\\n",
       "62       7/19/2018 8:26:52 AM -04:00   \n",
       "10696   7/19/2018 11:51:57 PM -04:00   \n",
       "10801    7/19/2018 6:27:03 AM -04:00   \n",
       "13312    7/19/2018 1:35:59 PM -04:00   \n",
       "16299   7/19/2018 11:15:42 PM -04:00   \n",
       "...                              ...   \n",
       "119207  7/19/2018 11:45:25 AM -04:00   \n",
       "23450    7/19/2018 7:05:52 AM -04:00   \n",
       "44755    7/19/2018 8:46:10 PM -04:00   \n",
       "172391   7/19/2018 2:43:18 PM -04:00   \n",
       "57828    7/20/2018 3:05:12 AM -04:00   \n",
       "\n",
       "                                                    title  \\\n",
       "62      FBI Chief Threatens To Quit If Trump Invites R...   \n",
       "10696   At least 8 reported dead as duck boat sinks ne...   \n",
       "10801   Ukraineâ€™s not a country, Putin told Bush. What...   \n",
       "13312   Through the looking glass: Democrats attack \"R...   \n",
       "16299   Sheriff: 8 people dead after Missouri tourist ...   \n",
       "...                                                   ...   \n",
       "119207  Roger Federer lost his iconic 'RF' logo when h...   \n",
       "23450                      diglos76 (u/diglos76) - Reddit   \n",
       "44755   Mike Terrell: Manistee River offers small stre...   \n",
       "172391  T-storms coming back onto Chicagoâ€™s weather sc...   \n",
       "57828   Tire comes off van on Ohio interstate, crashes...   \n",
       "\n",
       "                                                     text lang_reliability  \\\n",
       "62      by Knave Dave - Jul 18, 2018 1:11 pm ### This ...                1   \n",
       "10696   At least 8 reported dead as duck boat sinks ne...                1   \n",
       "10801   PostEverything Perspective ### Perspective Int...                1   \n",
       "13312   Through the looking glass: Democrats attack â€œR...                1   \n",
       "16299   Sheriff: 8 people dead after Missouri tourist ...                1   \n",
       "...                                                   ...              ...   \n",
       "119207  Lifestyle ### * Copyright Â© 2018 Business Insi...                1   \n",
       "23450   Press J to jump to the feed. Press question ma...                1   \n",
       "44755   A few passing clouds. Low 68F. Winds light and...                1   \n",
       "172391  T-storms coming back onto Chicagoâ€™s weather sc...                1   \n",
       "57828   ### Thank you for registering! ### We have sen...                1   \n",
       "\n",
       "       lang_iso  title_len  text_len     label  \n",
       "62           en         78      2858  helsinki  \n",
       "10696        en         90       856  duckboat  \n",
       "10801        en         79      8487  helsinki  \n",
       "13312        en         86      4440  helsinki  \n",
       "16299        en         68       766  duckboat  \n",
       "...         ...        ...       ...       ...  \n",
       "119207       en        107      1720       NaN  \n",
       "23450        en         30      6605       NaN  \n",
       "44755        en         80      4808       NaN  \n",
       "172391       en        170       248       NaN  \n",
       "57828        en         95       339       NaN  \n",
       "\n",
       "[10140 rows x 10 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tniyomkarn/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/tniyomkarn/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "    \n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "## text normzalization\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text_lemma(text):\n",
    "    \"\"\"\n",
    "        text: a paragrapy\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"[^\\w\\s]\", '', text) ## Remove all non-word characters (everything except numbers and letters)\n",
    "    text = text.lower() # lowercase text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwords from text\n",
    "    text_list = nltk.word_tokenize(text)\n",
    "    lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in text_list]) # lemmatization\n",
    "    #text_list2 = nltk.word_tokenize(lemmatized_output)\n",
    "    #stemmed_output = ' '.join([porter.stem(w) for w in text_list2]) #stemming\n",
    "    return lemmatized_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df['clean_text'] = sample_df.apply(lambda row: clean_text_lemma(row['text']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import codecs\n",
    "# from datetime import datetime\n",
    "# import vocabulary\n",
    "# import time\n",
    "# from numpy.random import choice\n",
    "# import numpy\n",
    "# import functools\n",
    "# import sys\n",
    "# import nltk\n",
    "\n",
    "# sys.setrecursionlimit(10000)\n",
    "\n",
    "\n",
    "# class HDP_gibbs_sampling:\n",
    "#     def __init__(self, K0=10, alpha=0.5, beta=0.5,gamma=1.5, docs= None, V= None):\n",
    "#         self.maxnn = 1\n",
    "#         self.alss=[] # an array for keep the stirling(N,1:N) for saving time consumming\n",
    "#         self.K = K0  # initial number of topics\n",
    "#         self.alpha = alpha # parameter of topics prior\n",
    "#         self.beta = beta   # parameter of words prior\n",
    "#         self.gamma = gamma # parameter of tables prior\n",
    "#         self.docs = docs # a list of documents which include the words\n",
    "#         self.V = V # number of different words in the vocabulary\n",
    "#         self.z_m_n = {} # topic assignements for documents\n",
    "#         self.n_m_z = numpy.zeros((len(self.docs), self.K))      # number of words assigned to topic z in document m\n",
    "#         self.theta = numpy.zeros((len(self.docs), self.K))\n",
    "#         self.n_z_t = numpy.zeros((self.K, V)) # number of times a word v is assigned to a topic z\n",
    "#         self.phi = numpy.zeros((self.K, V))\n",
    "#         self.n_z = numpy.zeros(self.K)   # total number of words assigned to a topic z\n",
    "#         self.U1=[] # active topics\n",
    "#         for i in range (self.K):\n",
    "#             self.U1.append(i)\n",
    "        \n",
    "#         self.U0=[] # deactive topics\n",
    "#         self.tau=numpy.zeros(self.K+1) +1./self.K\n",
    "#         for m, doc in enumerate(docs):         # Initialization of the data structures\n",
    "#             for n,t in enumerate(doc):\n",
    "#                 z = numpy.random.randint(0, self.K) # Randomly assign a topic to a word and increase the counting array\n",
    "#                 self.n_m_z[m, z] += 1\n",
    "#                 self.n_z_t[z, t] += 1\n",
    "#                 self.n_z[z] += 1\n",
    "#                 self.z_m_n[(m,n)]=z\n",
    "    \n",
    "\n",
    "#     def inference(self,iteration):\n",
    "#         \" Inference of HDP  using Dircet Assignment with ILDA simpilifying \"\n",
    "        \n",
    "#         for m, doc in enumerate(self.docs):\n",
    "#                 for n, t in enumerate(doc):\n",
    "#                     # decrease the counting for word t with topic kold\n",
    "#                     kold =self.z_m_n[(m,n)]\n",
    "#                     self.n_m_z[m,kold] -= 1\n",
    "#                     self.n_z_t[kold, t] -= 1\n",
    "#                     self.n_z[kold] -= 1\n",
    "#                     p_z=numpy.zeros(self.K+1)\n",
    "#                     for kk in range (self.K): # using the z sampling equation in ILDA\n",
    "#                         k=self.U1[kk]\n",
    "#                         p_z[kk]=(self.n_m_z[m,k]+self.alpha*self.tau[k])*(self.n_z_t[k,t]+self.beta)/(self.n_z[k]+self.V*self.beta)\n",
    "#                     p_z[self.K]=(self.alpha*self.tau[self.K])/self.V # additional cordinate for new topic\n",
    "#                     knew = numpy.random.multinomial(1, p_z / p_z.sum()).argmax()\n",
    "#                     if knew==self.K: # check if topic sample is new\n",
    "#                         self.z_m_n[(m,n)] = self.spawntopic(m,t) # extend the number of topics and arrays shape and assign the array for new topic\n",
    "#                         self.updatetau() # update the table distribution over topic\n",
    "\n",
    "\n",
    "#                     else :\n",
    "#                         k=self.U1[knew] # do same as LDA\n",
    "#                         self.z_m_n[(m,n)] = k\n",
    "#                         self.n_m_z[m,k] += 1\n",
    "#                         self.n_z_t[k, t] += 1\n",
    "#                         self.n_z[k] += 1\n",
    "                    \n",
    "                    \n",
    "#                     if self.n_z[kold]==0: # check if the topic have been not used and re shape the arrayes\n",
    "#                         self.U1.remove(kold)\n",
    "#                         self.U0.append(kold)\n",
    "#                         self.K -=1\n",
    "#                         self.updatetau()\n",
    "\n",
    "                \n",
    "\n",
    "#         print ('Iteration:',iteration,'\\n','Number of topics:',self.K,'\\n','Activated topics:',self.U1,'\\n','Deactivated topics',self.U0)\n",
    "\n",
    "\n",
    "#     def spawntopic (self,m,t): # reshape the arrays for new topic\n",
    "#         if len(self.U0)>0: # if the we have deactive topics.\n",
    "#             k=self.U0[0]\n",
    "#             self.U0.remove(k)\n",
    "#             self.U1.append(k)\n",
    "#             self.n_m_z[m,k]=1\n",
    "#             self.n_z_t[k,t]=1\n",
    "#             self.n_z[k]=1\n",
    "            \n",
    "            \n",
    "#         else:\n",
    "#             k=self.K #  if the we do not have deactive topics so far.\n",
    "#             self.n_m_z=numpy.append(self.n_m_z,numpy.zeros([len(self.docs),1]),1)\n",
    "#             self.U1.append(k)\n",
    "#             self.n_m_z[m,k] = 1\n",
    "#             self.n_z_t=numpy.vstack([self.n_z_t,numpy.zeros(self.V)])\n",
    "#             self.n_z_t[k, t] = 1\n",
    "#             self.n_z=numpy.append(self.n_z,1)\n",
    "#             self.tau=numpy.append(self.tau,0)\n",
    "        \n",
    "#         self.K +=1\n",
    "        \n",
    "#         return k\n",
    "    \n",
    "            \n",
    "#     def stirling(self,nn): # making an array for keep the stirling(N,1:N) for saving time consumming\n",
    "#         if len(self.alss)==0:\n",
    "#             self.alss.append([])\n",
    "#             self.alss[0].append(1)\n",
    "#         if nn > self.maxnn:\n",
    "#             for mm in range (self.maxnn,nn):\n",
    "#                 ln=len(self.alss[mm-1])+1\n",
    "#                 self.alss.append([])\n",
    "                \n",
    "#                 for xx in range(ln) :\n",
    "#                     self.alss[mm].append(0)\n",
    "#                     if xx< (ln-1):\n",
    "#                         self.alss[mm][xx] += self.alss[mm-1][xx]*mm\n",
    "#                     if xx>(ln-2) :\n",
    "#                         self.alss[mm][xx] += 0\n",
    "#                     if xx==0 :\n",
    "#                         self.alss[mm][xx] += 0\n",
    "#                     if xx!=0 :\n",
    "#                         self.alss[mm][xx] += self.alss[mm-1][xx-1]\n",
    "\n",
    "#             self.maxnn=nn\n",
    "#         return self.alss[nn-1]\n",
    "    \n",
    "    \n",
    "    \n",
    "#     def rand_antoniak(self,alpha, n):\n",
    "#         # Sample from Antoniak Distribution\n",
    "#         ss = self.stirling(n)\n",
    "#         max_val = max(ss)\n",
    "#         p = numpy.array(ss) / max_val\n",
    "        \n",
    "#         aa = 1\n",
    "#         for i, _ in enumerate(p):\n",
    "#             p[i] *= aa\n",
    "#             aa *= alpha\n",
    "        \n",
    "#         p = numpy.array(p,dtype='float') / numpy.array(p,dtype='float').sum()\n",
    "#         return choice(range(1, n+1), p=p)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     def updatetau(self):  # update tau using antoniak sampling from CRM\n",
    "    \n",
    "#         m_k=numpy.zeros(self.K+1)\n",
    "#         for kk in range(self.K):\n",
    "#             k=self.U1[kk]\n",
    "#             for m in range(len(self.docs)):\n",
    "                \n",
    "#                 if self.n_m_z[m,k]>1 :\n",
    "#                     m_k[kk]+=self.rand_antoniak(self.alpha*self.tau[k], int(self.n_m_z[m,k]))\n",
    "#                 else :\n",
    "#                     m_k[kk]+=self.n_m_z[m,k]\n",
    "    \n",
    "#         T=sum(m_k)\n",
    "#         m_k[self.K]=self.gamma\n",
    "#         tt=numpy.transpose(numpy.random.dirichlet(m_k, 1))\n",
    "#         for kk in range(self.K):\n",
    "#             k=self.U1[kk]\n",
    "#             self.tau[k]=tt[kk]\n",
    "\n",
    "#         self.tau[self.K]=tt[self.K]\n",
    "\n",
    "\n",
    "\n",
    "#     def worddist(self):\n",
    "#         \"\"\"topic-word distribution, \\phi in Blei'spaper  \"\"\"\n",
    "#         return (self.n_z_t +self.beta)/ (self.n_z[:, numpy.newaxis]+self.V*self.beta),len(self.n_z)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     corpus = sample_df['text']\n",
    "#     iterations = 50 # number of iterations for getting converged\n",
    "#     voca = vocabulary.Vocabulary(excluds_stopwords=False) # find the unique words in the dataset\n",
    "#     docs = [voca.doc_to_ids(doc) for doc in corpus] # change words of the corpus to ids\n",
    "#     HDP = HDP_gibbs_sampling(K0=20, alpha=0.5, beta=0.5, gamma=2, docs=docs, V=voca.size()) # initialize the HDP\n",
    "#     for i in range(iterations):\n",
    "#         HDP.inference(i)\n",
    "#     (d,len) = HDP.worddist() # find word distribution of each topic\n",
    "#     for i in range(len):\n",
    "#         ind = numpy.argpartition(d[i], -10)[-10:] # top 10 most occured words for each topic\n",
    "#         for j in ind:\n",
    "#             print (voca[j],' ',end=\"\"),\n",
    "#         print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "from datetime import datetime\n",
    "import vocabulary\n",
    "import time\n",
    "from numpy.random import choice\n",
    "import numpy\n",
    "import functools\n",
    "import sys\n",
    "import nltk\n",
    "\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "\n",
    "class HDP_gibbs_sampling:\n",
    "    def __init__(self, K0=10, alpha=0.5, beta=0.5,gamma=1.5, docs= None, V= None):\n",
    "        self.maxnn = 1\n",
    "        self.alss=[] # an array for keep the stirling(N,1:N) for saving time consumming\n",
    "        self.K = K0  # initial number of topics\n",
    "        self.alpha = alpha # parameter of topics prior\n",
    "        self.beta = beta   # parameter of words prior\n",
    "        self.gamma = gamma # parameter of tables prior\n",
    "        self.docs = docs # a list of documents which include the words\n",
    "        self.V = V # number of different words in the vocabulary\n",
    "        self.z_m_n = {} # topic assignements for documents\n",
    "        self.n_m_z = numpy.zeros((len(self.docs), self.K))      # number of words assigned to topic z in document m\n",
    "        self.theta = numpy.zeros((len(self.docs), self.K))\n",
    "        self.n_z_t = numpy.zeros((self.K, V)) # number of times a word v is assigned to a topic z\n",
    "        self.phi = numpy.zeros((self.K, V))\n",
    "        self.n_z = numpy.zeros(self.K)   # total number of words assigned to a topic z\n",
    "        self.U1=[] # active topics\n",
    "        for i in range (self.K):\n",
    "            self.U1.append(i)\n",
    "        \n",
    "        self.U0=[] # deactive topics\n",
    "        self.tau=numpy.zeros(self.K+1) +1./self.K\n",
    "        for m, doc in enumerate(docs):         # Initialization of the data structures\n",
    "            for n,t in enumerate(doc):\n",
    "                z = numpy.random.randint(0, self.K) # Randomly assign a topic to a word and increase the counting array\n",
    "                self.n_m_z[m, z] += 1\n",
    "                self.n_z_t[z, t] += 1\n",
    "                self.n_z[z] += 1\n",
    "                self.z_m_n[(m,n)]=z\n",
    "    \n",
    "\n",
    "    def inference(self,iteration):\n",
    "        \" Inference of HDP  using Dircet Assignment with ILDA simpilifying \"\n",
    "        \n",
    "        for m, doc in enumerate(self.docs):\n",
    "                for n, t in enumerate(doc):\n",
    "                    # decrease the counting for word t with topic kold\n",
    "                    kold =self.z_m_n[(m,n)]\n",
    "                    self.n_m_z[m,kold] -= 1\n",
    "                    self.n_z_t[kold, t] -= 1\n",
    "                    self.n_z[kold] -= 1\n",
    "                    p_z=numpy.zeros(self.K+1)\n",
    "                    for kk in range (self.K): # using the z sampling equation in ILDA\n",
    "                        k=self.U1[kk]\n",
    "                        p_z[kk]=(self.n_m_z[m,k]+self.alpha*self.tau[k])*(self.n_z_t[k,t]+self.beta)/(self.n_z[k]+self.V*self.beta)\n",
    "                    p_z[self.K]=(self.alpha*self.tau[self.K])/self.V # additional cordinate for new topic\n",
    "                    knew = numpy.random.multinomial(1, p_z / p_z.sum()).argmax()\n",
    "                    if knew==self.K: # check if topic sample is new\n",
    "                        self.z_m_n[(m,n)] = self.spawntopic(m,t) # extend the number of topics and arrays shape and assign the array for new topic\n",
    "                        self.updatetau() # update the table distribution over topic\n",
    "\n",
    "\n",
    "                    else :\n",
    "                        k=self.U1[knew] # do same as LDA\n",
    "                        self.z_m_n[(m,n)] = k\n",
    "                        self.n_m_z[m,k] += 1\n",
    "                        self.n_z_t[k, t] += 1\n",
    "                        self.n_z[k] += 1\n",
    "                    \n",
    "                    \n",
    "                    if self.n_z[kold]==0: # check if the topic have been not used and re shape the arrayes\n",
    "                        self.U1.remove(kold)\n",
    "                        self.U0.append(kold)\n",
    "                        self.K -=1\n",
    "                        self.updatetau()\n",
    "\n",
    "                \n",
    "\n",
    "        print ('Iteration:',iteration,'\\n','Number of topics:',self.K,'\\n','Activated topics:',self.U1,'\\n','Deactivated topics',self.U0)\n",
    "\n",
    "\n",
    "    def spawntopic (self,m,t): # reshape the arrays for new topic\n",
    "        if len(self.U0)>0: # if the we have deactive topics.\n",
    "            k=self.U0[0]\n",
    "            self.U0.remove(k)\n",
    "            self.U1.append(k)\n",
    "            self.n_m_z[m,k]=1\n",
    "            self.n_z_t[k,t]=1\n",
    "            self.n_z[k]=1\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            k=self.K #  if the we do not have deactive topics so far.\n",
    "            self.n_m_z=numpy.append(self.n_m_z,numpy.zeros([len(self.docs),1]),1)\n",
    "            self.U1.append(k)\n",
    "            self.n_m_z[m,k] = 1\n",
    "            self.n_z_t=numpy.vstack([self.n_z_t,numpy.zeros(self.V)])\n",
    "            self.n_z_t[k, t] = 1\n",
    "            self.n_z=numpy.append(self.n_z,1)\n",
    "            self.tau=numpy.append(self.tau,0)\n",
    "        \n",
    "        self.K +=1\n",
    "        \n",
    "        return k\n",
    "    \n",
    "            \n",
    "    def stirling(self,nn): # making an array for keep the stirling(N,1:N) for saving time consumming\n",
    "        if len(self.alss)==0:\n",
    "            self.alss.append([])\n",
    "            self.alss[0].append(1)\n",
    "        if nn > self.maxnn:\n",
    "            for mm in range (self.maxnn,nn):\n",
    "                ln=len(self.alss[mm-1])+1\n",
    "                self.alss.append([])\n",
    "                \n",
    "                for xx in range(ln) :\n",
    "                    self.alss[mm].append(0)\n",
    "                    if xx< (ln-1):\n",
    "                        self.alss[mm][xx] += self.alss[mm-1][xx]*mm\n",
    "                    if xx>(ln-2) :\n",
    "                        self.alss[mm][xx] += 0\n",
    "                    if xx==0 :\n",
    "                        self.alss[mm][xx] += 0\n",
    "                    if xx!=0 :\n",
    "                        self.alss[mm][xx] += self.alss[mm-1][xx-1]\n",
    "\n",
    "            self.maxnn=nn\n",
    "        return self.alss[nn-1]\n",
    "    \n",
    "    \n",
    "    \n",
    "    def rand_antoniak(self,alpha, n):\n",
    "        # Sample from Antoniak Distribution\n",
    "        ss = self.stirling(n)\n",
    "        max_val = max(ss)\n",
    "        p = numpy.array(ss) / max_val\n",
    "        \n",
    "        aa = 1\n",
    "        for i, _ in enumerate(p):\n",
    "            p[i] *= aa\n",
    "            aa *= alpha\n",
    "        \n",
    "        p = numpy.array(p,dtype='float') / numpy.array(p,dtype='float').sum()\n",
    "        return choice(range(1, n+1), p=p)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def updatetau(self):  # update tau using antoniak sampling from CRM\n",
    "    \n",
    "        m_k=numpy.zeros(self.K+1)\n",
    "        for kk in range(self.K):\n",
    "            k=self.U1[kk]\n",
    "            for m in range(len(self.docs)):\n",
    "                \n",
    "                if self.n_m_z[m,k]>1 :\n",
    "                    m_k[kk]+=self.rand_antoniak(self.alpha*self.tau[k], int(self.n_m_z[m,k]))\n",
    "                else :\n",
    "                    m_k[kk]+=self.n_m_z[m,k]\n",
    "    \n",
    "        T=sum(m_k)\n",
    "        m_k[self.K]=self.gamma\n",
    "        tt=numpy.transpose(numpy.random.dirichlet(m_k, 1))\n",
    "        for kk in range(self.K):\n",
    "            k=self.U1[kk]\n",
    "            self.tau[k]=tt[kk]\n",
    "\n",
    "        self.tau[self.K]=tt[self.K]\n",
    "\n",
    "\n",
    "\n",
    "    def worddist(self):\n",
    "        \"\"\"topic-word distribution, \\phi in Blei'spaper  \"\"\"\n",
    "        return (self.n_z_t +self.beta)/ (self.n_z[:, numpy.newaxis]+self.V*self.beta),len(self.n_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \n",
      " Number of topics: 21 \n",
      " Activated topics: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20] \n",
      " Deactivated topics []\n",
      "Iteration: 1 \n",
      " Number of topics: 20 \n",
      " Activated topics: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] \n",
      " Deactivated topics [20]\n",
      "Iteration: 2 \n",
      " Number of topics: 20 \n",
      " Activated topics: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] \n",
      " Deactivated topics [20]\n",
      "Iteration: 3 \n",
      " Number of topics: 20 \n",
      " Activated topics: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] \n",
      " Deactivated topics [20]\n",
      "Iteration: 4 \n",
      " Number of topics: 20 \n",
      " Activated topics: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] \n",
      " Deactivated topics [20]\n",
      "Iteration: 5 \n",
      " Number of topics: 20 \n",
      " Activated topics: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] \n",
      " Deactivated topics [20]\n",
      "Iteration: 6 \n",
      " Number of topics: 20 \n",
      " Activated topics: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] \n",
      " Deactivated topics [20]\n",
      "Iteration: 7 \n",
      " Number of topics: 20 \n",
      " Activated topics: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] \n",
      " Deactivated topics [20]\n",
      "Iteration: 8 \n",
      " Number of topics: 20 \n",
      " Activated topics: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] \n",
      " Deactivated topics [20]\n",
      "Iteration: 9 \n",
      " Number of topics: 20 \n",
      " Activated topics: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] \n",
      " Deactivated topics [20]\n",
      "Iteration: 10 \n",
      " Number of topics: 20 \n",
      " Activated topics: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] \n",
      " Deactivated topics [20]\n",
      "Iteration: 11 \n",
      " Number of topics: 20 \n",
      " Activated topics: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] \n",
      " Deactivated topics [20]\n",
      "Iteration: 12 \n",
      " Number of topics: 20 \n",
      " Activated topics: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] \n",
      " Deactivated topics [20]\n",
      "Iteration: 13 \n",
      " Number of topics: 20 \n",
      " Activated topics: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] \n",
      " Deactivated topics [20]\n",
      "Iteration: 14 \n",
      " Number of topics: 20 \n",
      " Activated topics: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] \n",
      " Deactivated topics [20]\n",
      "Iteration: 15 \n",
      " Number of topics: 20 \n",
      " Activated topics: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] \n",
      " Deactivated topics [20]\n",
      "Iteration: 16 \n",
      " Number of topics: 20 \n",
      " Activated topics: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] \n",
      " Deactivated topics [20]\n",
      "Iteration: 17 \n",
      " Number of topics: 20 \n",
      " Activated topics: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] \n",
      " Deactivated topics [20]\n",
      "Iteration: 18 \n",
      " Number of topics: 20 \n",
      " Activated topics: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] \n",
      " Deactivated topics [20]\n",
      "Iteration: 19 \n",
      " Number of topics: 20 \n",
      " Activated topics: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] \n",
      " Deactivated topics [20]\n",
      "bay  pm  vehicle  officer  image  water  photo  police  said  july  \n",
      "pm  file  say  state  thursday  july  police  said  gmt  ap  \n",
      "like  keep  best  food  also  professional  find  home  pest  control  \n",
      "man  st  wednesday  family  image  july  photo  pm  new  park  \n",
      "community  child  health  year  new  school  people  work  also  said  \n",
      "news  thai  wednesday  said  thailand  team  coach  soccer  boy  cave  \n",
      "award  kim  medium  korea  specie  north  best  said  trump  korean  \n",
      "say  told  may  u  july  butina  school  police  said  court  \n",
      "state  would  city  home  new  year  july  percent  u  said  \n",
      "cancer  group  wind  force  u  said  year  july  people  government  \n",
      "july  show  comiccon  game  view  may  san  video  published  new  \n",
      "football  kirby  espys  team  lee  usa  today  game  sport  player  \n",
      "people  also  show  one  like  new  film  time  get  life  \n",
      "say  home  woman  july  family  car  dog  police  said  child  \n",
      "image  lake  show  people  july  said  boat  photo  fire  wednesday  \n",
      "state  house  election  russian  russia  president  putin  u  trump  said  \n",
      "share  million  also  market  business  new  company  said  billion  year  \n",
      "time  open  last  team  season  first  one  year  said  game  \n",
      "law  new  access  said  people  one  right  court  would  state  \n",
      "people  know  say  get  time  day  one  like  year  said  \n",
      "braking  perfecting  beguiling  powertrain  gleclass  crosswind  authorised  autonomously  parktronic  sluggishmoving  \n"
     ]
    }
   ],
   "source": [
    "corpus = sample_df['clean_text']\n",
    "iterations = 20 # number of iterations for getting converged\n",
    "voca = vocabulary.Vocabulary(excluds_stopwords=False) # find the unique words in the dataset\n",
    "docs = [voca.doc_to_ids(doc) for doc in corpus] # change words of the corpus to ids\n",
    "HDP = HDP_gibbs_sampling(K0=20, alpha=0.5, beta=0.5, gamma=2, docs=docs, V=voca.size()) # initialize the HDP\n",
    "for i in range(iterations):\n",
    "    HDP.inference(i)\n",
    "(d,length) = HDP.worddist() # find word distribution of each topic\n",
    "for i in range(length):\n",
    "    ind = numpy.argpartition(d[i], -10)[-10:] # top 10 most occured words for each topic\n",
    "    for j in ind:\n",
    "        print (voca[j],' ',end=\"\"),\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62        knave dave jul 18 2018 111 pm exactly first st...\n",
       "10696     least 8 reported dead duck boat sink near bran...\n",
       "10801     posteverything perspective perspective interpr...\n",
       "13312     looking glass democrat attack russian bear cha...\n",
       "16299     sheriff 8 people dead missouri tourist boat ac...\n",
       "                                ...                        \n",
       "119207    lifestyle copyright 2018 business insider inc ...\n",
       "23450     press j jump feed press question mark learn re...\n",
       "44755     passing cloud low 68f wind light variable toni...\n",
       "172391    tstorms coming back onto chicago weather scene...\n",
       "57828     thank registering sent confirmation email data...\n",
       "Name: clean_text, Length: 10140, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df['clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62        by Knave Dave - Jul 18, 2018 1:11 pm ### This ...\n",
       "10696     At least 8 reported dead as duck boat sinks ne...\n",
       "10801     PostEverything Perspective ### Perspective Int...\n",
       "13312     Through the looking glass: Democrats attack â€œR...\n",
       "16299     Sheriff: 8 people dead after Missouri tourist ...\n",
       "                                ...                        \n",
       "119207    Lifestyle ### * Copyright Â© 2018 Business Insi...\n",
       "23450     Press J to jump to the feed. Press question ma...\n",
       "44755     A few passing clouds. Low 68F. Winds light and...\n",
       "172391    T-storms coming back onto Chicagoâ€™s weather sc...\n",
       "57828     ### Thank you for registering! ### We have sen...\n",
       "Name: text, Length: 10140, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
