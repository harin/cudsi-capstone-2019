{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readArticle(input_file_path):\n",
    "    \"\"\"\n",
    "        file path: where the input file located \n",
    "        \n",
    "        return: a dataframe\n",
    "    \"\"\"\n",
    "    file = open(input_file_path, 'r', encoding = 'utf8')\n",
    "    articles_dt = file.read().split('\\n')[:-1]\n",
    "    pubId, canonicalUrl,firstScrape,title,text,lang_reliability = [],[],[],[],[],[]\n",
    "    for article in articles_dt:    \n",
    "        row = article.split('\\t')\n",
    "        pubId.append(row[0])\n",
    "        canonicalUrl.append(row[4])\n",
    "        firstScrape.append(row[5])\n",
    "        lang_reliability.append(row[7])\n",
    "        title.append(row[8])\n",
    "        text.append(row[9])\n",
    "    articles_df = pd.DataFrame()\n",
    "    articles_df['pubId'], articles_df['canonicalUrl'], articles_df['firstScrape'], articles_df['title'], articles_df['text'], articles_df['lang_reliability']= pubId, canonicalUrl,firstScrape,title,text,lang_reliability\n",
    "    return articles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## text normzalization\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a article \n",
    "        \n",
    "        return: normalized the text data\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"[^\\w\\s]\", '', text) ## Remove all non-word characters (everything except numbers and letters)\n",
    "    text = text.lower() # lowercase text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwords from text\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = readArticle('../data/raw/articles.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(213605, 6)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>canonicalUrl</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>bbc.com/news/av/world-asia-44875089/thai-cave-...</td>\n",
       "      <td>cave_rescue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>indystar.com/story/news/nation-now/2018/07/16/...</td>\n",
       "      <td>cave_rescue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>washingtonpost.com/world/asia_pacific/these-di...</td>\n",
       "      <td>cave_rescue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>au.news.yahoo.com/navy-seal-died-thai-cave-res...</td>\n",
       "      <td>cave_rescue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>yahoo.com/news/m/8adca8cd-6cc3-307c-b109-9cd1d...</td>\n",
       "      <td>cave_rescue</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        canonicalUrl        label\n",
       "0  bbc.com/news/av/world-asia-44875089/thai-cave-...  cave_rescue\n",
       "1  indystar.com/story/news/nation-now/2018/07/16/...  cave_rescue\n",
       "2  washingtonpost.com/world/asia_pacific/these-di...  cave_rescue\n",
       "3  au.news.yahoo.com/navy-seal-died-thai-cave-res...  cave_rescue\n",
       "4  yahoo.com/news/m/8adca8cd-6cc3-307c-b109-9cd1d...  cave_rescue"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the label data\n",
    "label1 = pd.read_csv('../data/raw/labels/cave_rescue/lower_bound.txt', header=None)\n",
    "label1.columns = ['canonicalUrl']\n",
    "label1['label'] = 'cave_rescue'\n",
    "label2 = pd.read_csv('../data/raw/labels/duckboat/lower_bound.txt', header=None)\n",
    "label2.columns = ['canonicalUrl']\n",
    "label2['label'] = 'duckboat'\n",
    "label3 = pd.read_csv('../data/raw/labels/helsinki_summit/lower_bound.txt', header=None)\n",
    "label3.columns = ['canonicalUrl']\n",
    "label3['label'] = 'helsinki'\n",
    "label_df = pd.concat([label1, label2, label3])\n",
    "label_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(141, 7)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#merged df and the label_df\n",
    "merged_df = df.merge(label_df, on='canonicalUrl', how='left')\n",
    "#drop na labels\n",
    "merged_df = merged_df.dropna(subset=['label'])\n",
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>pubId</th>\n",
       "      <th>canonicalUrl</th>\n",
       "      <th>firstScrape</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>lang_reliability</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>count</th>\n",
       "      <th>count</th>\n",
       "      <th>count</th>\n",
       "      <th>count</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>cave_rescue</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>duckboat</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>helsinki</td>\n",
       "      <td>89</td>\n",
       "      <td>89</td>\n",
       "      <td>89</td>\n",
       "      <td>89</td>\n",
       "      <td>89</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            pubId canonicalUrl firstScrape title  text lang_reliability\n",
       "            count        count       count count count            count\n",
       "label                                                                  \n",
       "cave_rescue    31           31          31    31    31               31\n",
       "duckboat       21           21          21    21    21               21\n",
       "helsinki       89           89          89    89    89               89"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#need to generate balanced data \n",
    "merged_df.groupby(['label']).agg(['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(213464, 6)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we need 859 artiels so from each categroy \n",
    "sub = df[~df['canonicalUrl'].isin(merged_df['canonicalUrl'])]\n",
    "sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211436, 6)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = sub[sub['lang_reliability'] == '1']\n",
    "sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_df = sub[['canonicalUrl', 'text']].sample(859)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df[['canonicalUrl', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df = pd.concat([merged_df, rest_df])\n",
    "sample_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.to_csv('../data/embedding/sample_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random select samples\n",
    "text_df = merged_df['text'].apply(clean_text)\n",
    "sample_text = text_df.sample(n = 10000, random_state = 2, replace=True) \n",
    "sample_label = merged_df.loc[sample_text.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cave_rescue', 'duckboat', 'helsinki']\n"
     ]
    }
   ],
   "source": [
    "#label_encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(sample_label['label'])\n",
    "print(list(le.classes_))\n",
    "sample_y = le.transform(sample_label['label']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the noun, verb, word entity for each articles\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "noun_phrases_list = []\n",
    "verb_phrases_list = []\n",
    "entites_list = []\n",
    "for text in sample_text:\n",
    "    doc = nlp(text)\n",
    "    noun_phrases_list.append([chunk.text for chunk in doc.noun_chunks])\n",
    "    verb_phrases_list.append([token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "    entites_list.append([entity.text for entity in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_list(input_lists):\n",
    "    \"\"\"\n",
    "        input_lists: a list of string lists\n",
    "        \n",
    "        return: a list of string\n",
    "    \"\"\"\n",
    "    str_list = []\n",
    "    #join the verb_list in to string\n",
    "    for l in input_lists:\n",
    "        str_list.append(' '.join(l))\n",
    "    return str_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding the entites BoW\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "def BoW_embedding(input_list):\n",
    "    #unigram\n",
    "    \"\"\"        \n",
    "        input_list: a list of string\n",
    "        return: embedding output\n",
    "    \"\"\"\n",
    "    cv = CountVectorizer()\n",
    "    text_counts= cv.fit_transform(input_list)\n",
    "    return text_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classified with SVM\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "def SVM(X,y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    clf = SVC(gamma='auto')\n",
    "    clf.fit(X_train, y_train) \n",
    "    y_pred = clf.predict(X_test)\n",
    "    print('accuracy_score', accuracy_score(y_test, y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1: \n",
    "#### get the noun from the articles> embedding the noun(BoW) > classified the artciles with the baseline (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score 0.7875757575757576\n"
     ]
    }
   ],
   "source": [
    "#noun \n",
    "string_list = format_list(noun_phrases_list)\n",
    "X = BoW_embedding(string_list)\n",
    "SVM(X,sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2: \n",
    "#### get the verb from the articles> embedding the verbs(BoW) > classified the artciles with the baseline (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score 0.7363636363636363\n"
     ]
    }
   ],
   "source": [
    "#entities: entites_list\n",
    "string_list = format_list(verb_phrases_list)\n",
    "X = BoW_embedding(string_list)\n",
    "SVM(X,sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 3: \n",
    "#### get the entites from the articles> embedding the entites (BoW)> classified the artciles with the baseline (SVM )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score 0.7336363636363636\n"
     ]
    }
   ],
   "source": [
    "#entities: entites_list\n",
    "string_list = format_list(entites_list)\n",
    "X = BoW_embedding(string_list)\n",
    "SVM(X,sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the entity output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(213610, 7)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#merged df and the label_df\n",
    "merged_df = df.merge(label_df, on='canonicalUrl', how='left')\n",
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(213610,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df = merged_df['text'].apply(clean_text)\n",
    "text_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the eneties for the whole data:text_df\n",
    "#get the noun, verb, word entity for each articles\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "noun_phrases_list = []\n",
    "verb_phrases_list = []\n",
    "entites_list = []\n",
    "for text in text_df:\n",
    "    doc = nlp(text)\n",
    "    noun_phrases_list.append([chunk.text for chunk in doc.noun_chunks])\n",
    "    verb_phrases_list.append([token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "    entites_list.append([entity.text for entity in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "213610"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(noun_phrases_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the entites\n",
    "with open('../data/embedding/noun_phrases.txt', 'w') as f:\n",
    "    for item in noun_phrases_list:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/embedding/verb_phrase.txt', 'w') as f:\n",
    "    for item in verb_phrases_list:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/embedding/entites.txt', 'w') as f:\n",
    "    for item in entites_list:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['phoenix capita',\n",
       " '19',\n",
       " '2018',\n",
       " '849',\n",
       " '1 trillion',\n",
       " 'next year',\n",
       " '19',\n",
       " '2018',\n",
       " '849',\n",
       " 'uk',\n",
       " 'two',\n",
       " 'uk',\n",
       " '19',\n",
       " '801 neil mitchell',\n",
       " 'uk',\n",
       " 'uk',\n",
       " '19',\n",
       " '801 0',\n",
       " '43bn 5 billion',\n",
       " 'eu',\n",
       " 'us',\n",
       " 'european union',\n",
       " 'five billion dollar',\n",
       " 'one',\n",
       " 'july 19 2018',\n",
       " 'wednesday',\n",
       " 'alphabet',\n",
       " 'browser apps',\n",
       " 'eu',\n",
       " '5',\n",
       " 'netherlands',\n",
       " 'chinese',\n",
       " 'us',\n",
       " 'european unions',\n",
       " 'ftc',\n",
       " 'joseph simons',\n",
       " 'congress',\n",
       " 'wednesday',\n",
       " 'eu',\n",
       " 'several years ago',\n",
       " 'trumps comments',\n",
       " 'european commission',\n",
       " 'jeanclaude junckers',\n",
       " 'washington',\n",
       " 'next week',\n",
       " 'russia',\n",
       " 'nato',\n",
       " 'eu']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entites_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
