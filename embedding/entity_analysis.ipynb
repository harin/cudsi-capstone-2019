{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readArticle(input_file_path):\n",
    "    \"\"\"\n",
    "        file path: where the input file located \n",
    "        \n",
    "        return: a dataframe\n",
    "    \"\"\"\n",
    "    file = open(input_file_path, 'r', encoding = 'utf8')\n",
    "    articles_dt = file.read().split('\\n')[:-1]\n",
    "    pubId, canonicalUrl,firstScrape,title,text,lang_reliability = [],[],[],[],[],[]\n",
    "    for article in articles_dt:    \n",
    "        row = article.split('\\t')\n",
    "        pubId.append(row[0])\n",
    "        canonicalUrl.append(row[4])\n",
    "        firstScrape.append(row[5])\n",
    "        lang_reliability.append(row[7])\n",
    "        title.append(row[8])\n",
    "        text.append(row[9])\n",
    "    articles_df = pd.DataFrame()\n",
    "    articles_df['pubId'], articles_df['canonicalUrl'], articles_df['firstScrape'], articles_df['title'], articles_df['text'], articles_df['lang_reliability']= pubId, canonicalUrl,firstScrape,title,text,lang_reliability\n",
    "    return articles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## text normzalization\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a article \n",
    "        \n",
    "        return: normalized the text data\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"[^\\w\\s]\", '', text) ## Remove all non-word characters (everything except numbers and letters)\n",
    "    text = text.lower() # lowercase text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwords from text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = readArticle('../data/raw/articles.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>canonicalUrl</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>denverpost.com/2018/07/20/greeley-police-lip-s...</td>\n",
       "      <td>cave_rescue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>bbc.com/news/av/world-asia-44875089/thai-cave-...</td>\n",
       "      <td>cave_rescue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>news.google.com/stories/caaqzggkimbdqkltuwpvsm...</td>\n",
       "      <td>cave_rescue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>chron.com/news/crime/article/young-nubians-rev...</td>\n",
       "      <td>cave_rescue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>cnbc.com/2018/07/20/at-least-13-die-when-duck-...</td>\n",
       "      <td>cave_rescue</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        canonicalUrl        label\n",
       "0  denverpost.com/2018/07/20/greeley-police-lip-s...  cave_rescue\n",
       "1  bbc.com/news/av/world-asia-44875089/thai-cave-...  cave_rescue\n",
       "2  news.google.com/stories/caaqzggkimbdqkltuwpvsm...  cave_rescue\n",
       "3  chron.com/news/crime/article/young-nubians-rev...  cave_rescue\n",
       "4  cnbc.com/2018/07/20/at-least-13-die-when-duck-...  cave_rescue"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the label data\n",
    "label1 = pd.read_csv('../data/raw/labels/cave_rescue/population.txt', header=None)\n",
    "label1.columns = ['canonicalUrl']\n",
    "label1['label'] = 'cave_rescue'\n",
    "label2 = pd.read_csv('../data/raw/labels/duckboat/population.txt', header=None)\n",
    "label2.columns = ['canonicalUrl']\n",
    "label2['label'] = 'duckboat'\n",
    "label3 = pd.read_csv('../data/raw/labels/helsinki_summit/population.txt', header=None)\n",
    "label3.columns = ['canonicalUrl']\n",
    "label3['label'] = 'helsinki'\n",
    "label_df = pd.concat([label1, label2, label3])\n",
    "label_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pubId</th>\n",
       "      <th>canonicalUrl</th>\n",
       "      <th>firstScrape</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>lang_reliability</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>290</td>\n",
       "      <td>zerohedge.com/news/2018-07-19/la-liberals-stag...</td>\n",
       "      <td>7/19/2018 7:50:19 PM -04:00</td>\n",
       "      <td>LA Liberals Stage \"Emergency Protest\" At Koshe...</td>\n",
       "      <td>by Phoenix Capita… - Jul 19, 2018 8:49 am ### ...</td>\n",
       "      <td>1</td>\n",
       "      <td>helsinki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>290</td>\n",
       "      <td>zerohedge.com/news/2018-07-19/wells-fargo-has-...</td>\n",
       "      <td>7/19/2018 11:53:37 AM -04:00</td>\n",
       "      <td>Wells Fargo Caught In Yet Another Scandal | Ze...</td>\n",
       "      <td>by Phoenix Capita… - Jul 19, 2018 8:49 am ### ...</td>\n",
       "      <td>1</td>\n",
       "      <td>helsinki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>290</td>\n",
       "      <td>zerohedge.com/news/2018-07-19/white-house-aske...</td>\n",
       "      <td>7/20/2018 1:35:15 AM -04:00</td>\n",
       "      <td>Asked 8 Times For Trump-Rouhani Meeting, Iran...</td>\n",
       "      <td>by Phoenix Capita… - Jul 19, 2018 8:49 am ### ...</td>\n",
       "      <td>1</td>\n",
       "      <td>helsinki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>290</td>\n",
       "      <td>zerohedge.com/news/2018-07-18/wheres-panic-why...</td>\n",
       "      <td>7/19/2018 8:26:52 AM -04:00</td>\n",
       "      <td>\"Where’s The Panic\": Why Trade War Hedges Aren...</td>\n",
       "      <td>by Knave Dave - Jul 18, 2018 1:11 pm ### This ...</td>\n",
       "      <td>1</td>\n",
       "      <td>helsinki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>290</td>\n",
       "      <td>zerohedge.com/news/2018-07-19/maxine-waters-fe...</td>\n",
       "      <td>7/19/2018 1:03:04 PM -04:00</td>\n",
       "      <td>Maxine Waters Fears \"Armed Protests\" As Oath K...</td>\n",
       "      <td>by Phoenix Capita… - Jul 19, 2018 8:49 am ### ...</td>\n",
       "      <td>1</td>\n",
       "      <td>cave_rescue</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pubId                                       canonicalUrl  \\\n",
       "12   290  zerohedge.com/news/2018-07-19/la-liberals-stag...   \n",
       "23   290  zerohedge.com/news/2018-07-19/wells-fargo-has-...   \n",
       "30   290  zerohedge.com/news/2018-07-19/white-house-aske...   \n",
       "34   290  zerohedge.com/news/2018-07-18/wheres-panic-why...   \n",
       "53   290  zerohedge.com/news/2018-07-19/maxine-waters-fe...   \n",
       "\n",
       "                     firstScrape  \\\n",
       "12   7/19/2018 7:50:19 PM -04:00   \n",
       "23  7/19/2018 11:53:37 AM -04:00   \n",
       "30   7/20/2018 1:35:15 AM -04:00   \n",
       "34   7/19/2018 8:26:52 AM -04:00   \n",
       "53   7/19/2018 1:03:04 PM -04:00   \n",
       "\n",
       "                                                title  \\\n",
       "12  LA Liberals Stage \"Emergency Protest\" At Koshe...   \n",
       "23  Wells Fargo Caught In Yet Another Scandal | Ze...   \n",
       "30   Asked 8 Times For Trump-Rouhani Meeting, Iran...   \n",
       "34  \"Where’s The Panic\": Why Trade War Hedges Aren...   \n",
       "53  Maxine Waters Fears \"Armed Protests\" As Oath K...   \n",
       "\n",
       "                                                 text lang_reliability  \\\n",
       "12  by Phoenix Capita… - Jul 19, 2018 8:49 am ### ...                1   \n",
       "23  by Phoenix Capita… - Jul 19, 2018 8:49 am ### ...                1   \n",
       "30  by Phoenix Capita… - Jul 19, 2018 8:49 am ### ...                1   \n",
       "34  by Knave Dave - Jul 18, 2018 1:11 pm ### This ...                1   \n",
       "53  by Phoenix Capita… - Jul 19, 2018 8:49 am ### ...                1   \n",
       "\n",
       "          label  \n",
       "12     helsinki  \n",
       "23     helsinki  \n",
       "30     helsinki  \n",
       "34     helsinki  \n",
       "53  cave_rescue  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#merged df and the label_df\n",
    "merged_df = df.merge(label_df, on='canonicalUrl', how='left')\n",
    "#drop na labels\n",
    "merged_df = merged_df.dropna(subset=['label'])\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random select samples\n",
    "text_df = merged_df['text'].apply(clean_text)\n",
    "sample_text = text_df.sample(n = 10000, random_state = 2, replace=True) \n",
    "sample_label = merged_df.loc[sample_text.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cave_rescue', 'duckboat', 'helsinki']\n"
     ]
    }
   ],
   "source": [
    "#label_encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(sample_label['label'])\n",
    "print(list(le.classes_))\n",
    "sample_y = le.transform(sample_label['label']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the noun, verb, word entity for each articles\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "noun_phrases_list = []\n",
    "verb_phrases_list = []\n",
    "entites_list = []\n",
    "for text in sample_text:\n",
    "    doc = nlp(text)\n",
    "    noun_phrases_list.append([chunk.text for chunk in doc.noun_chunks])\n",
    "    verb_phrases_list.append([token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "    entites_list.append([entity.text for entity in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_list(input_lists):\n",
    "    \"\"\"\n",
    "        input_lists: a list of string lists\n",
    "        \n",
    "        return: a list of string\n",
    "    \"\"\"\n",
    "    str_list = []\n",
    "    #join the verb_list in to string\n",
    "    for l in input_lists:\n",
    "        str_list.append(' '.join(l))\n",
    "    return str_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding the entites BoW\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "def BoW_embedding(input_list):\n",
    "    #unigram\n",
    "    \"\"\"        \n",
    "        input_list: a list of string\n",
    "        return: embedding output\n",
    "    \"\"\"\n",
    "    cv = CountVectorizer()\n",
    "    text_counts= cv.fit_transform(input_list)\n",
    "    return text_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classified with SVM\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "def SVM(X,y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    clf = SVC(gamma='auto')\n",
    "    clf.fit(X_train, y_train) \n",
    "    y_pred = clf.predict(X_test)\n",
    "    print('accuracy_score', accuracy_score(y_test, y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1: \n",
    "#### get the noun from the articles> embedding the noun(BoW) > classified the artciles with the baseline (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score 0.7875757575757576\n"
     ]
    }
   ],
   "source": [
    "#noun \n",
    "string_list = format_list(noun_phrases_list)\n",
    "X = BoW_embedding(string_list)\n",
    "SVM(X,sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2: \n",
    "#### get the verb from the articles> embedding the verbs(BoW) > classified the artciles with the baseline (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score 0.7363636363636363\n"
     ]
    }
   ],
   "source": [
    "#entities: entites_list\n",
    "string_list = format_list(verb_phrases_list)\n",
    "X = BoW_embedding(string_list)\n",
    "SVM(X,sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 3: \n",
    "#### get the entites from the articles> embedding the entites (BoW)> classified the artciles with the baseline (SVM )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score 0.7336363636363636\n"
     ]
    }
   ],
   "source": [
    "#entities: entites_list\n",
    "string_list = format_list(entites_list)\n",
    "X = BoW_embedding(string_list)\n",
    "SVM(X,sample_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
