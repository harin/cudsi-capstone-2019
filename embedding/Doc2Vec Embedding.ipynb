{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec Embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Cleaning Step:\n",
    "1.\tDelete unreliable articles\n",
    "2.\tDelete stopwords (using NLTK package)\n",
    "3.\tRemove non-word characters: only keep letters and numbers\n",
    "4.\tLower all letters\n",
    "5.  Concat Title and Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('2018_07_19_04_59_08/articles.txt', 'r', encoding = 'utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#articles.txt is a file containing article records, one record per row, column definition is:\n",
    "columns = [\"pubId\", \"is_hourly\", \"seqId\", \"on_homepage\", \"canonicalUrl\",\n",
    "                   \"firstScrape\", \"lang_iso\", \"lang_reliability\", \"title\", \"text\"]\n",
    "articles_dt = file.read().split('\\n')[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "pubId, canonicalUrl,firstScrape,title,text,lang_reliability = [],[],[],[],[],[]\n",
    "for article in articles_dt:    \n",
    "    row = article.split('\\t')\n",
    "    pubId.append(row[0])\n",
    "    canonicalUrl.append(row[4])\n",
    "    firstScrape.append(row[5])\n",
    "    lang_reliability.append(row[7])\n",
    "    title.append(row[8])\n",
    "    text.append(row[9])\n",
    "articles_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df['pubId'], articles_df['canonicalUrl'], articles_df['firstScrape'], articles_df['title'], articles_df['text'], articles_df['lang_reliability']= pubId, canonicalUrl,firstScrape,title,text,lang_reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(213605, 6)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211577, 6)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#exclude the lang_reliability = 0 which the detected language is not reliable\n",
    "articles_df = articles_df[articles_df['lang_reliability'] == '1']\n",
    "articles_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df['title_text'] = articles_df['title'] + ' '+ articles_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "## text normzalization\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a paragrapy\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"[^\\w\\s]\", '', text) ## Remove all non-word characters (everything except numbers and letters)\n",
    "    text = text.lower() # lowercase text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwords from text\n",
    "    return text\n",
    "    \n",
    "articles_df['text'] = articles_df['text'].apply(clean_text)\n",
    "articles_df['title_text'] = articles_df['title_text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_text_df = articles_df[['title_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = articles_df[['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df.to_csv('text_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_text_df.to_csv('title_text_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the dependencies\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini = text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = [Tagge dDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(text_df.text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "211577"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tagged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 20\n",
    "vec_size = 20\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "  \n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print('iteration {0}'.format(epoch))\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"d2v.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04742454 -3.9067266   0.6358228   0.75249374  1.7423975   0.9739114\n",
      "  0.7258888  -0.41517594  3.0489283  -2.9445236  -0.28707018 -0.79701144\n",
      "  4.063946    1.6476473  -0.5928256   1.7204026  -4.7911644   1.914418\n",
      "  1.8326075   0.27115056 -0.75462824 -2.2386963   1.6526892  -1.9546459\n",
      "  0.7244203   1.4253081  -0.5431972  -1.7073781  -0.61894894 -3.1310725\n",
      "  2.6362584   2.584425    1.6833615  -0.25219202  0.9889768   1.0735435\n",
      "  1.9781995   2.0766425   2.4369762  -1.143024   -0.46532598 -0.02445841\n",
      " -0.50257665  4.0439854   2.68998     1.2311356  -1.826763   -0.9050933\n",
      "  0.93818873 -0.7991395   0.54384965  0.55795205 -1.343361   -1.3555621\n",
      "  1.5750258   3.9228294   0.5858833  -2.3397343   1.7796873   2.248494\n",
      " -0.36234215  0.8671693  -2.360166    0.05404849 -3.21477    -2.087069\n",
      "  2.2954795   0.81484246 -1.8066285   1.2576919   0.9409692   1.4169419\n",
      "  5.494096   -1.8293006   1.9247261   0.23652166  3.796841    2.6306183\n",
      " -0.6738159  -2.817919    1.8070489   1.5071226   1.7859106   0.01357475\n",
      "  0.21653233  2.217431   -3.3597515   1.9794234  -1.0247108  -1.6655276\n",
      "  3.9682968   0.8031037  -1.0497011  -2.6465638   0.84725314 -2.600662\n",
      " -0.7528322   0.02889317 -0.42416584 -1.4459552 ]\n"
     ]
    }
   ],
   "source": [
    "print(model.docvecs[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "list_of_lists = []\n",
    "for i in range(text_df.shape[0]):\n",
    "    list_of_lists.append(model.docvecs[i])\n",
    "\n",
    "d2v_vec20 = pd.DataFrame(list_of_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_vec20.to_csv('./result/d2v_vec100.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**vec_size=100**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/gensim/models/doc2vec.py:574: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 20\n",
    "vec_size = 100\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "  \n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print('iteration {0}'.format(epoch))\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"d2v.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "list_of_lists = []\n",
    "for i in range(text_df.shape[0]):\n",
    "    list_of_lists.append(model.docvecs[i])\n",
    "\n",
    "d2v_vec100 = pd.DataFrame(list_of_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_vec100.to_csv('./result/d2v_vec100.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concat title and text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(title_text_df.title_text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2vec(max_epochs, vec_size, alpha, tagged_data):\n",
    "    model = Doc2Vec(size=vec_size,\n",
    "                    alpha=alpha, \n",
    "                    min_alpha=0.00025,\n",
    "                    min_count=1,\n",
    "                    dm =1)\n",
    "\n",
    "    model.build_vocab(tagged_data)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        print('iteration {0}'.format(epoch))\n",
    "        model.train(tagged_data,\n",
    "                    total_examples=model.corpus_count,\n",
    "                    epochs=model.iter)\n",
    "        # decrease the learning rate\n",
    "        model.alpha -= 0.0002\n",
    "        # fix the learning rate, no decay\n",
    "        model.min_alpha = model.alpha\n",
    "\n",
    "    model.save(\"d2v.model\")\n",
    "    print(\"Model Saved\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "model_titletext = doc2vec(20, 100, 0.025, tagged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04742454 -3.9067266   0.6358228   0.75249374  1.7423975   0.9739114\n",
      "  0.7258888  -0.41517594  3.0489283  -2.9445236  -0.28707018 -0.79701144\n",
      "  4.063946    1.6476473  -0.5928256   1.7204026  -4.7911644   1.914418\n",
      "  1.8326075   0.27115056 -0.75462824 -2.2386963   1.6526892  -1.9546459\n",
      "  0.7244203   1.4253081  -0.5431972  -1.7073781  -0.61894894 -3.1310725\n",
      "  2.6362584   2.584425    1.6833615  -0.25219202  0.9889768   1.0735435\n",
      "  1.9781995   2.0766425   2.4369762  -1.143024   -0.46532598 -0.02445841\n",
      " -0.50257665  4.0439854   2.68998     1.2311356  -1.826763   -0.9050933\n",
      "  0.93818873 -0.7991395   0.54384965  0.55795205 -1.343361   -1.3555621\n",
      "  1.5750258   3.9228294   0.5858833  -2.3397343   1.7796873   2.248494\n",
      " -0.36234215  0.8671693  -2.360166    0.05404849 -3.21477    -2.087069\n",
      "  2.2954795   0.81484246 -1.8066285   1.2576919   0.9409692   1.4169419\n",
      "  5.494096   -1.8293006   1.9247261   0.23652166  3.796841    2.6306183\n",
      " -0.6738159  -2.817919    1.8070489   1.5071226   1.7859106   0.01357475\n",
      "  0.21653233  2.217431   -3.3597515   1.9794234  -1.0247108  -1.6655276\n",
      "  3.9682968   0.8031037  -1.0497011  -2.6465638   0.84725314 -2.600662\n",
      " -0.7528322   0.02889317 -0.42416584 -1.4459552 ]\n"
     ]
    }
   ],
   "source": [
    "print(model.docvecs[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "list_of_lists = []\n",
    "for i in range(text_df.shape[0]):\n",
    "    list_of_lists.append(model.docvecs[i])\n",
    "\n",
    "d2v_vec100_titletext = pd.DataFrame(list_of_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211577, 100)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2v_vec100_titletext.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_vec100.to_csv('./result/d2v_vec100_titletext.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
