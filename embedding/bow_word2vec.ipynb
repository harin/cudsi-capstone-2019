{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(213605, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open('articles.txt', 'r', encoding = 'utf8')\n",
    "columns = [\"pubId\", \"is_hourly\", \"seqId\", \"on_homepage\", \"canonicalUrl\",\n",
    "                   \"firstScrape\", \"lang_iso\", \"lang_reliability\", \"title\", \"text\"]\n",
    "\n",
    "\n",
    "articles_dt = file.read().split('\\n')[:-1]\n",
    "pubId, is_hourly, seqId, on_homepage, canonicalUrl,firstScrape,lang_iso, lang_reliability, title,text = [],[],[],[],[], [],[],[],[],[]\n",
    "for article in articles_dt:    \n",
    "    row = article.split('\\t')\n",
    "    pubId.append(row[0])\n",
    "    is_hourly.append(row[1])\n",
    "    seqId.append(row[2])\n",
    "    on_homepage.append(row[3])\n",
    "    canonicalUrl.append(row[4])\n",
    "    firstScrape.append(row[5])\n",
    "    lang_iso.append(row[6])\n",
    "    lang_reliability.append(row[7])\n",
    "    title.append(row[8])\n",
    "    text.append(row[9])\n",
    "articles_df = pd.DataFrame()\n",
    "articles_df['pubId'], articles_df['is_hourly'], articles_df['seqId'], articles_df['on_homepage'], articles_df['canonicalUrl'], articles_df['firstScrape'], articles_df['lang_iso'], articles_df['lang_reliability'],articles_df['title'], articles_df['text'] = pubId, is_hourly, seqId, on_homepage, canonicalUrl,firstScrape,lang_iso, lang_reliability, title,text\n",
    "articles_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211577, 10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_df = articles_df[articles_df['lang_reliability'] == '1']\n",
    "articles_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## text normzalization\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import re\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a paragrapy\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"[^\\w\\s]\", '', text) ## Remove all non-word characters (everything except numbers and letters)\n",
    "    text = text.lower() # lowercase text\n",
    "    text_list = nltk.word_tokenize(text)\n",
    "    lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in text_list]) # lemmatization\n",
    "    return lemmatized_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df['text'] = articles_df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df['title'] = articles_df['title'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pubId</th>\n",
       "      <th>is_hourly</th>\n",
       "      <th>seqId</th>\n",
       "      <th>on_homepage</th>\n",
       "      <th>canonicalUrl</th>\n",
       "      <th>firstScrape</th>\n",
       "      <th>lang_iso</th>\n",
       "      <th>lang_reliability</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>290</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>zerohedge.com/news/2018-07-19/i-told-you-so-tr...</td>\n",
       "      <td>7/19/2018 10:41:26 AM -04:00</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>i told you so trump hit out at eu after 5 bill...</td>\n",
       "      <td>by phoenix caput jul 19 2018 849 am the govern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>290</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>zerohedge.com/news/2018-07-19/trump-invites-pu...</td>\n",
       "      <td>7/19/2018 5:03:01 PM -04:00</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>trump invite putin to ahead of midterm zero hedge</td>\n",
       "      <td>by phoenix caput jul 19 2018 849 am the govern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>290</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>zerohedge.com/news/2018-07-19/gallup-shows-how...</td>\n",
       "      <td>7/19/2018 8:26:52 AM -04:00</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>gallup show how much american really care abou...</td>\n",
       "      <td>by knave dave jul 18 2018 111 pm this is exact...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>290</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>zerohedge.com/news/2018-07-19/california-supre...</td>\n",
       "      <td>7/19/2018 8:26:52 AM -04:00</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>california supreme court block proposal to spl...</td>\n",
       "      <td>by knave dave jul 18 2018 111 pm this is exact...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>290</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>zerohedge.com/news/2018-07-19/why-are-thousand...</td>\n",
       "      <td>7/19/2018 4:28:52 PM -04:00</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>why are thousand of tesla sitting in a field i...</td>\n",
       "      <td>by phoenix caput jul 19 2018 849 am the govern...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pubId is_hourly seqId on_homepage  \\\n",
       "0   290         1     2           1   \n",
       "1   290         1     4           1   \n",
       "2   290         1     5           1   \n",
       "3   290         1     7           1   \n",
       "4   290         1     9           1   \n",
       "\n",
       "                                        canonicalUrl  \\\n",
       "0  zerohedge.com/news/2018-07-19/i-told-you-so-tr...   \n",
       "1  zerohedge.com/news/2018-07-19/trump-invites-pu...   \n",
       "2  zerohedge.com/news/2018-07-19/gallup-shows-how...   \n",
       "3  zerohedge.com/news/2018-07-19/california-supre...   \n",
       "4  zerohedge.com/news/2018-07-19/why-are-thousand...   \n",
       "\n",
       "                    firstScrape lang_iso lang_reliability  \\\n",
       "0  7/19/2018 10:41:26 AM -04:00       en                1   \n",
       "1   7/19/2018 5:03:01 PM -04:00       en                1   \n",
       "2   7/19/2018 8:26:52 AM -04:00       en                1   \n",
       "3   7/19/2018 8:26:52 AM -04:00       en                1   \n",
       "4   7/19/2018 4:28:52 PM -04:00       en                1   \n",
       "\n",
       "                                               title  \\\n",
       "0  i told you so trump hit out at eu after 5 bill...   \n",
       "1  trump invite putin to ahead of midterm zero hedge   \n",
       "2  gallup show how much american really care abou...   \n",
       "3  california supreme court block proposal to spl...   \n",
       "4  why are thousand of tesla sitting in a field i...   \n",
       "\n",
       "                                                text  \n",
       "0  by phoenix caput jul 19 2018 849 am the govern...  \n",
       "1  by phoenix caput jul 19 2018 849 am the govern...  \n",
       "2  by knave dave jul 18 2018 111 pm this is exact...  \n",
       "3  by knave dave jul 18 2018 111 pm this is exact...  \n",
       "4  by phoenix caput jul 19 2018 849 am the govern...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bag of word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. uni-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb = []\n",
    "for i in articles_df.index:\n",
    "    comb.append(articles_df['title'][i] + ' ' + articles_df['text'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb[712]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#title_text = pd.DataFrame(comb) \n",
    "#title_text.to_csv(r'title_text_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "text_counts= cv.fit_transform(comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211577, 807149)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(text_counts[712,:].todense())\n",
    "# empty context+title is 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save result\n",
    "np.save('BagOfWord_addlemmatization_output.npy', text_counts)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved result\n",
    "#test = np.load('BagOfWord_output.npy', allow_pickle=True)\n",
    "#np.asmatrix(test)[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. bi-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2 = CountVectorizer(ngram_range=(2, 2))\n",
    "text_counts2= cv2.fit_transform(comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211577, 9470591)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_counts2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(text_counts2[712,:].todense())\n",
    "# empty context+title is 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('BagOfWord2_addlemmatization_output.npy', text_counts)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec,KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec(text): \n",
    "    article_vect = np.empty([len(text), 300])\n",
    "    for i in range(0,len(text)):\n",
    "        word_list = text[i].split(' ')\n",
    "        for word in word_list:\n",
    "            total_vec = np.zeros(300)\n",
    "            if word in model:\n",
    "                vec = model[word]\n",
    "                total_vec = total_vec + vec\n",
    "        article_vect[i,:] = total_vec/len(word_list)\n",
    "    return article_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_vect = word2vec(comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211577, 300)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(article_vect[712,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('word2vec_addlemmatization_output.npy', article_vect) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
